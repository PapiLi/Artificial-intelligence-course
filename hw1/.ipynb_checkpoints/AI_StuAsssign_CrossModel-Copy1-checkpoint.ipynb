{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_hparam_string(OrderNum,lr,BatchNum,CV,DataScaleFlag):\n",
    "    return('Order_'+str(OrderNum)+'_lr_'+str(lr)+'_BatchNum_'+str(BatchNum)+'_CV_'+str(TestingDataRatio)+'_Flag_'+str(DataScaleFlag))\n",
    "\n",
    "def GenInputMatrix(x,n):\n",
    "    i = 2\n",
    "    y = x\n",
    "    while i<=n:\n",
    "        y = np.hstack((np.power(x,i), y))\n",
    "        i = i+1\n",
    "    return(y)\n",
    "\n",
    "def DataScaling(X_train,X_test,flag):\n",
    "    if flag == 1:\n",
    "        E_X = np.sum(X_train,0) / X_train.shape[0]\n",
    "        E_X.shape=1,-1\n",
    "        D_X = np.sqrt(np.sum(np.square(X_train - np.tile(E_X,[X_train.shape[0],1])),0)) / X_train.shape[0]\n",
    "        X_train_Norm = (X_train - np.tile(E_X,[X_train.shape[0],1])) / np.tile(D_X,[X_train.shape[0],1])\n",
    "        X_test_Norm = (X_test - np.tile(E_X, [X_test.shape[0], 1])) / np.tile(D_X,[X_test.shape[0],1])\n",
    "        return (X_train_Norm,X_test_Norm,E_X,D_X)\n",
    "    else:\n",
    "        return (X_train,X_test,np.zeros([1,X_train.shape[1]]),np.ones([1,X_train.shape[1]]))\n",
    "\n",
    "def Predict(X,W,b):\n",
    "    # Hypothesis:Linear model\n",
    "    b=b.reshape(1,1)\n",
    "    b_expand = np.tile(b, [X.shape[0], 1])\n",
    "#    print(W,b)\n",
    "#    print(X.shape,b_expand.shape)\n",
    "    return(np.matmul(X, W)+b_expand)\n",
    "\n",
    "# Hyperparameters\n",
    "training_epochs = int(1e20)\n",
    "#learning rate lr\n",
    "lr = 1*1e-1\n",
    "#total sample number N\n",
    "N = 500\n",
    "#TestingSamplesNumber/TotalSamplesNumber:TestingDataRatio\n",
    "TestingDataRatio=0.2\n",
    "#the highest order of linear model:OrderNumcd\n",
    "OrderNum=3\n",
    "#Use Student Number as seed\n",
    "Seed = 16225151\n",
    "#DataScalingFlag=1 indicates to use data scaling before fitting\n",
    "DataScaleFlag=1\n",
    "BatchSize = int(N * (1-TestingDataRatio))\n",
    "BatchNum = int(N * (1-TestingDataRatio)/BatchSize)\n",
    "#Train_LogDir = \"C:/PyLogs/LinearRegress/Train_GD_\" + make_hparam_string(OrderNum,lr,BatchNum,TestingDataRatio,DataScaleFlag)\n",
    "#Test_LogDir = \"C:/PyLogs/LinearRegress/Test_GD_\" + make_hparam_string(OrderNum,lr,BatchNum,TestingDataRatio,DataScaleFlag)\n",
    "Train_LogDir = \"C:/PyLogs/LinearRegress/Train_Adam_\" + make_hparam_string(OrderNum,lr,BatchNum,TestingDataRatio,DataScaleFlag)\n",
    "Test_LogDir = \"C:/PyLogs/LinearRegress/Test_Adam_\" + make_hparam_string(OrderNum,lr,BatchNum,TestingDataRatio,DataScaleFlag)\n",
    "\n",
    "np.random.seed(Seed)\n",
    "tf.set_random_seed(Seed)\n",
    "#Construct N data samples(including training data set and testing data set)\n",
    "x = np.linspace(0, 6, N) + np.random.randn(N)\n",
    "x = np.sort(x)\n",
    "y = x ** 2 - 4 * x - 3 + np.random.randn(N)\n",
    "x.shape = -1, 1\n",
    "y.shape = -1, 1\n",
    "\n",
    "#training dataset and testing dataset: x -> [x**2 x]\n",
    "x_expand=GenInputMatrix(x,OrderNum)\n",
    "#print(x_expand)\n",
    "\n",
    "x_train0, x_test0, y_train, y_test = train_test_split(x_expand, y, test_size = TestingDataRatio, random_state=Seed)\n",
    "#print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n",
    "\n",
    "#Input Data Scaling for training and testing procedure\n",
    "[x_train,x_test,x_expand_E,x_expand_D] = DataScaling(x_train0,x_test0,DataScaleFlag)\n",
    "#print(x_expand_E,x_expand_D)\n",
    "\n",
    "# placeholders for a tensor of a Linear model with 2-order.\n",
    "X = tf.placeholder(tf.float32, shape=[None, OrderNum])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "with tf.name_scope(\"Parameters\") as scope:\n",
    "    W = tf.Variable(tf.random_normal([OrderNum, 1]), name='weight')\n",
    "    b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis:Linear model\n",
    "with tf.name_scope(\"Hypothesis\") as scope:\n",
    "    hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\") as scope:\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    Cost_summ = tf.summary.scalar(\"Batch_Cost\", cost)\n",
    "    Total_cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    Total_cost_summ = tf.summary.scalar(\"Total_cost\", Total_cost)\n",
    "\n",
    "\n",
    "# Minimize\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "#    optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "#    optimizer = tf.train.MomentumOptimizer(learning_rate=lr, momentum = 0.9)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "#    optimizer = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "#    optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, momentum = 0.9)\n",
    "    train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    Train_writer = tf.summary.FileWriter(Train_LogDir)\n",
    "    Test_writer = tf.summary.FileWriter(Test_LogDir)\n",
    "    Train_writer.add_graph(sess.graph)  # Show the graph\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Initial w,b:',sess.run([W,b]))\n",
    "    print('TrainingDataNum|TestingDataNum=',N * (1-TestingDataRatio),'|',N * TestingDataRatio)\n",
    "    print(\"fit with \"+str(OrderNum)+\"-order linear model...\")\n",
    "\n",
    "    prev_cost = sess.run(cost, feed_dict={X: x_train, Y: y_train})\n",
    "    Initial_test_cost = sess.run(cost, feed_dict={X: x_test, Y: y_test})\n",
    "    #print(prev_cost)\n",
    "    Train_cost_history=[prev_cost]\n",
    "    Test_cost_history=[Initial_test_cost]\n",
    "    OuterLoopFlag=0\n",
    "    Train_writer.add_summary(sess.run(Total_cost_summ, feed_dict={X: x_train, Y: y_train}), 0)\n",
    "    Test_writer.add_summary(sess.run(Total_cost_summ, feed_dict={X: x_test, Y: y_test}), 0)\n",
    "    time_start = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "       for j in range(BatchNum):\n",
    "            StartIndex=int(j*BatchSize)\n",
    "            EndIndex = int((j+1) * BatchSize)\n",
    "            Batch_error, _ = sess.run([cost,train], feed_dict={X: x_train[StartIndex:EndIndex,:], Y: y_train[StartIndex:EndIndex,:]})\n",
    "            Train_writer.add_summary(sess.run(Cost_summ, feed_dict={X: x_train, Y: y_train}), BatchNum*epoch+j)\n",
    "            Test_writer.add_summary(sess.run(Cost_summ, feed_dict={X: x_test, Y: y_test}), BatchNum*epoch+j)\n",
    "            curr_cost = sess.run(cost, feed_dict={X: x_train, Y: y_train})\n",
    "#            print(prev_cost,curr_cost)\n",
    "            Testing_cost = sess.run(cost, feed_dict={X: x_test, Y: y_test})\n",
    "            Train_cost_history.append(curr_cost)\n",
    "            Test_cost_history.append(Testing_cost)\n",
    "            if abs(curr_cost - prev_cost) < 1e-15:\n",
    "#            print('step:', step,prev_cost, curr_cost)\n",
    "                OuterLoopFlag = 1\n",
    "                break\n",
    "            prev_cost = curr_cost\n",
    "       Train_writer.add_summary(sess.run(Total_cost_summ, feed_dict={X: x_train, Y: y_train}), epoch+1)\n",
    "       Test_writer.add_summary(sess.run(Total_cost_summ, feed_dict={X: x_test, Y: y_test}), epoch+1)\n",
    "       if OuterLoopFlag == 1:\n",
    "           break\n",
    "    W_star,b_star = sess.run([W,b])\n",
    "\n",
    "time_end = time.time()\n",
    "\n",
    "W_star = W_star / x_expand_D.reshape([-1,1])\n",
    "b_star = b_star - np.sum(W_star * x_expand_E.reshape([-1,1]))\n",
    "print(u'最佳参数 W,b:',W_star,b_star)\n",
    "\n",
    "print(time_end - time_start, u'秒')\n",
    "print('Training...Cost=', curr_cost)\n",
    "# Error report on Training and Testing\n",
    "print('Testing...Cost=', Testing_cost)\n",
    "#    print(W_star,b_star)\n",
    "#    print(x_expand_E,x_expand_D)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(1,2,1)\n",
    "#print(x_train0.shape)\n",
    "plt.plot(x_train0[:,-1], y_train, 'ro', ms=5,label='training data')\n",
    "plt.plot(x_test0[:,-1], y_test, 'go', ms=5,label='testing data')\n",
    "x1 = np.linspace(-2, 10, 1000)\n",
    "x1.shape = -1, 1\n",
    "x1_expand=GenInputMatrix(x1,OrderNum)\n",
    "#print(x1_expand.shape, W_star.shape, b_star.shape)\n",
    "Est_y1 = Predict(x1_expand,W_star,b_star)\n",
    "plt.plot(x1,Est_y1,\"b--\",label='Fitting curve')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.title(\"fit with \"+str(OrderNum)+\"-order linear model\", fontsize=18)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('Y', fontsize=16)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "NoParam = np.arange(0,len(Train_cost_history),1)\n",
    "#print(NoParam,cost_history)\n",
    "plt.plot(NoParam, Train_cost_history, 'r-', ms=10,label='Training Learning Curve')\n",
    "plt.plot(NoParam, Test_cost_history, 'b-', ms=10,label='Testing Learning Curve')\n",
    "plt.xlabel('#PramUpdate', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curve', fontsize=18)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
